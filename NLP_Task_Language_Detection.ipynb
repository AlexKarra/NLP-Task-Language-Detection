{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoXdD90eeXw6ou1mfbXiDr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKarra/NLP-Task-Language-Detection/blob/main/NLP_Task_Language_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfH0DjvOBVpA",
        "outputId": "4391f3cb-7a15-4e92-e986-f530e34f6c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "                                                Text  language\n",
            "0  klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
            "1  sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
            "2  ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
            "3  விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
            "4  de spons behoort tot het geslacht haliclona en...     Dutch\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "dataset = pd.read_csv(\"/content/drive/My Drive/dataset.csv\")\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get an overview of missing data in the dataset\n",
        "\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgbOcE-DIEru",
        "outputId": "b612bc24-5344-4b99-dd44-c1294a616b1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text        0\n",
              "language    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the sentences of presented languages\n",
        "\n",
        "dataset[\"language\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajggzSsbIE7S",
        "outputId": "2464656d-763f-4447-ae9b-5c1c4d1815f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Estonian      1000\n",
              "Swedish       1000\n",
              "English       1000\n",
              "Russian       1000\n",
              "Romanian      1000\n",
              "Persian       1000\n",
              "Pushto        1000\n",
              "Spanish       1000\n",
              "Hindi         1000\n",
              "Korean        1000\n",
              "Chinese       1000\n",
              "French        1000\n",
              "Portugese     1000\n",
              "Indonesian    1000\n",
              "Urdu          1000\n",
              "Latin         1000\n",
              "Turkish       1000\n",
              "Japanese      1000\n",
              "Dutch         1000\n",
              "Tamil         1000\n",
              "Thai          1000\n",
              "Arabic        1000\n",
              "Name: language, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains 22 languages with 1000 sentences from each language. This is a very balanced dataset with no missing values, so we can say this dataset is completely ready to be used to train a machine learning model."
      ],
      "metadata": {
        "id": "wD3zMzzNp9Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array(dataset[\"Text\"])\n",
        "y = np.array(dataset[\"language\"])\n",
        "\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(x)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=60)"
      ],
      "metadata": {
        "id": "u7WAS7qOISMb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(X_train,y_train)\n",
        "model.score(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYPFhOyxISW1",
        "outputId": "0dc9ebbc-9fae-4af7-df49-be300e2e2597"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9556473829201102"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading the docx file with python-docx package\n",
        "\n",
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv4B_OqmX9wi",
        "outputId": "14e1073f-beb7-48a1-a767-7deadbe8bca6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.10.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "\n",
        "# Replace with the path to the .docx file\n",
        "docx_file_path = \"/content/drive/My Drive/Presentation GloVe my part.docx\"\n",
        "\n",
        "# Read the .docx file\n",
        "document = Document(docx_file_path)\n",
        "\n",
        "# Extract and print each paragraph\n",
        "for paragraph in document.paragraphs:\n",
        "  text = paragraph.text\n",
        "\n",
        "data = cv.transform([text]).toarray()\n",
        "output = model.predict(data)\n",
        "print(output)\n",
        "\n"
      ],
      "metadata": {
        "id": "ddllFKW4YFHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0614b701-08e8-4edb-821c-0c4cb4bb3276"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['English']\n"
          ]
        }
      ]
    }
  ]
}